{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    " \n",
    "import re\n",
    "from sys import exit\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import unicodedata\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "class Crawler():\n",
    "    \n",
    "    def __init__ (self):\n",
    "        pass\n",
    "        \n",
    "\n",
    "    def strip_punctuation(self,text):\n",
    "        if text is None:\n",
    "            return None\n",
    "        punctutation_cats = set(['Pc', 'Pd', 'Ps', 'Pe', 'Pi', 'Pf', 'Po'])\n",
    "        return  ''.join(x for x in text if unicodedata.category(x)\n",
    "                       not in punctutation_cats)\n",
    "\n",
    "    def scrape_data(self ,url  , path ,depth=4,flag=0 ,urls=[]):\n",
    "        path = \"./data/\"str(path)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        \n",
    "        base_url = urlparse(url).scheme+\"://\"+urlparse(url).netloc+\"/\"\n",
    "        \n",
    "        for t in urls:\n",
    "            if t['url'] == url:\n",
    "                if t['depth'] <= depth:\n",
    "                    return True\n",
    "        \n",
    "        urls.append({'url':url,'depth':depth})\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "            response = None\n",
    "            for i in range (3):\n",
    "                try:\n",
    "                    response = requests.get(url,headers=headers)\n",
    "                    break\n",
    "                except ConnectionError as e:\n",
    "                    print(e)\n",
    "                    if i == 2:\n",
    "                        return e\n",
    "            html = response.content\n",
    "            \n",
    "\n",
    "            if response.status_code == 200:\n",
    "                file_name = urlparse(url).netloc+\"/\"+urlparse(url).path.replace(\"../\",\"\").replace(\"/\",\"\")\n",
    "                file_name = file_name.replace(\"/\",\"\")\n",
    "\n",
    "                if path[-1] != \"/\":\n",
    "                    path+=\"/\"\n",
    "\n",
    "                if url[-5] != \".html\":\n",
    "                    file_name += \".html\"\n",
    "\n",
    "                \n",
    "\n",
    "                with open(path+file_name , 'wb') as f:\n",
    "                    f.write(html)\n",
    "                \n",
    "                \n",
    "\n",
    "                print(\"flag : \",flag)\n",
    "                print(\"url : \",url)\n",
    "                print(\"file name : \",path+file_name)\n",
    "                print('-'*30)\n",
    "\n",
    "                if flag < depth:\n",
    "                    soup = BeautifulSoup(html,'lxml')\n",
    "                    all_as = soup.findAll(\"a\")\n",
    "                    if all_as:\n",
    "                        for a in all_as:\n",
    "                            try:\n",
    "                                href = a['href']\n",
    "        #                         print(href)\n",
    "                                if href:\n",
    "                                    if href.find(\"://\") == -1:\n",
    "                                        if href[0] != \"#\":\n",
    "            #                                 print(base_url+href)\n",
    "                                            time.sleep(1)\n",
    "                                            self.scrape_data(base_url+href , path ,depth ,flag+1,urls)\n",
    "                                    elif urlparse(href).netloc == urlparse(url).netloc:\n",
    "        #                                 print(href)\n",
    "                                        time.sleep(1)\n",
    "                                        self.scrape_data(href , path ,depth ,flag+1,urls)\n",
    "                            except KeyError as e:\n",
    "                                print(e)\n",
    "                                pass\n",
    "                    else:\n",
    "                        print(\"no links\")\n",
    "        #                 print(all_as)\n",
    "            else:\n",
    "                print(\"status_code : \" , response.status_code)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flag :  0\n",
      "url :  http://nb.81.cn\n",
      "file name :  4/nb.81.cn.html\n",
      "------------------------------\n",
      "flag :  1\n",
      "url :  http://nb.81.cn/\n",
      "file name :  4/nb.81.cn.html\n",
      "------------------------------\n",
      "'href'\n",
      "'href'\n",
      "'href'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f0d53186d334>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscrape_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-0c74cbf781de>\u001b[0m in \u001b[0;36mscrape_data\u001b[0;34m(self, url, path, depth, flag, urls)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m#                                 print(href)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                                         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscrape_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhref\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mdepth\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-0c74cbf781de>\u001b[0m in \u001b[0;36mscrape_data\u001b[0;34m(self, url, path, depth, flag, urls)\u001b[0m\n\u001b[1;32m     87\u001b[0m                                         \u001b[0;32mif\u001b[0m \u001b[0mhref\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"#\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;31m#                                 print(base_url+href)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                                             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscrape_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mhref\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mdepth\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                                     \u001b[0;32melif\u001b[0m \u001b[0murlparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetloc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0murlparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetloc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_url = \"http://nb.81.cn\"\n",
    "id = 4\n",
    "\n",
    "c = Crawler()\n",
    "c.scrape_data(base_url,id,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = []\n",
    "\n",
    "# # domain_id = Domain(url=base_url)\n",
    "\n",
    "# for (dirpath, dirnames, filenames) in os.walk(\".data\"):#../patents data\n",
    "#     count = 0\n",
    "#     for file in filenames:\n",
    "#             files.append(file)\n",
    "#             html = \"\"\n",
    "#             with open(\"./data/\"+file , \"rb\") as f:\n",
    "#                 html = f.read()\n",
    "#             soup = BeautifulSoup(html,\"lxml\")\n",
    "#             all_ps = soup.findAll(\"p\")\n",
    "#             if all_ps:\n",
    "#                 for p in all_ps:\n",
    "#                     text = p.getText(strip=True)\n",
    "#                     # regix replace words with tag\n",
    "#                     # for each word found save keywords_pargraphs\n",
    "#                     for word in keywords:\n",
    "#                         if\n",
    "#                     html = re.sub('foo','bar', line.rstrip())\n",
    "#                     # save pargraph\n",
    "                \n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
